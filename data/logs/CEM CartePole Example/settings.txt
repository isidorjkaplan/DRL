Command Arguments: Namespace(agents_path='data/agents.txt', debug=False, ensemble_path='data/ensemble.txt', env='CartPole-v0')

Agents Configuration File:
	PolicyAgent --net=Linear --train=cem --model=linear-cem.pt
Ensemble Configuration File: RandomAgent

Trainer={'debug': True, 'agent': <src.agents.PolicyAgent object at 0x7f8ea77c9d00>, 'backup_file': 'data/logs/05_01_2021 13_33_50/models/linear-cem.pt', 'last_iter_time': 1609871630.5532029, 'plotter': <src.plotting.Plotter object at 0x7f8ea77870d0>, 'tag': 'PolicyAgent net_Linear train_cem model_linearcem_pt', 'iter_num': 0, 'save_every': 1, 'optimizer': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), 'batch': [], 'batch_size': 64, 'percentile': 90, 'objective': CrossEntropyLoss(), 'elite_buffer': False, 'buffer': [], 'episode_queue': <queue.Queue object at 0x7f8ea77c9c10>, 'batch_queue': <queue.Queue object at 0x7f8ea77c9f10>}
Agent (PolicyAgent --net=Linear --train=cem --model=linear-cem.pt): Agent={'net': Linear(
  (linear): Sequential(
    (0): Linear(in_features=4, out_features=64, bias=True)
    (1): ReLU()
  )
  (policy): Linear(in_features=64, out_features=2, bias=True)
  (value): Linear(in_features=64, out_features=1, bias=True)
  (advantages): Linear(in_features=64, out_features=2, bias=True)
), 'debug': False, 'tag': 'PolicyAgent net_Linear train_cem model_linearcem_pt', 'n_actions': 2, 'write_file': <_io.TextIOWrapper name='data/logs/05_01_2021 13_33_50/agent_logs/PolicyAgent net_Linear train_cem model_linearcem_pt.csv' mode='w' encoding='UTF-8'>, 'csv_writer': <_csv.writer object at 0x7f8ea7782360>, 'epsilon': 0}

Agent (RandomAgent): Agent={'n_actions': 1, 'tag': 'RandomAgent'}

